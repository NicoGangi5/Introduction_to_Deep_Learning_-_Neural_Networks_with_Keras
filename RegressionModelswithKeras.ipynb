{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RegressionModelswithKeras.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPnQD/ZlXU4Lyz/pZFHxLBq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NicoGangi5/Introduction_to_Deep_Learning_-_Neural_Networks_with_Keras/blob/main/RegressionModelswithKeras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhJ5hpXviQJF"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46F1WXoWlztV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5730529d-9fa8-4ceb-cca4-7d1125af0647"
      },
      "source": [
        "concrete_data = pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv')\n",
        "concrete_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "      <th>Strength</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1040.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "      <td>79.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1055.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "      <td>61.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>270</td>\n",
              "      <td>40.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>365</td>\n",
              "      <td>41.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>198.6</td>\n",
              "      <td>132.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>192.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>978.4</td>\n",
              "      <td>825.5</td>\n",
              "      <td>360</td>\n",
              "      <td>44.30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Cement  Blast Furnace Slag  Fly Ash  ...  Fine Aggregate  Age  Strength\n",
              "0   540.0                 0.0      0.0  ...           676.0   28     79.99\n",
              "1   540.0                 0.0      0.0  ...           676.0   28     61.89\n",
              "2   332.5               142.5      0.0  ...           594.0  270     40.27\n",
              "3   332.5               142.5      0.0  ...           594.0  365     41.05\n",
              "4   198.6               132.4      0.0  ...           825.5  360     44.30\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR0tqOX6l7bn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53c179e0-be6e-4c27-f925-28c4fc4f4f0a"
      },
      "source": [
        "concrete_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1030, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWjnjoYQmket",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "f04adafe-8c4e-4bc7-9f80-3ea0852e4f1f"
      },
      "source": [
        "concrete_data.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "      <th>Strength</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>281.167864</td>\n",
              "      <td>73.895825</td>\n",
              "      <td>54.188350</td>\n",
              "      <td>181.567282</td>\n",
              "      <td>6.204660</td>\n",
              "      <td>972.918932</td>\n",
              "      <td>773.580485</td>\n",
              "      <td>45.662136</td>\n",
              "      <td>35.817961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>104.506364</td>\n",
              "      <td>86.279342</td>\n",
              "      <td>63.997004</td>\n",
              "      <td>21.354219</td>\n",
              "      <td>5.973841</td>\n",
              "      <td>77.753954</td>\n",
              "      <td>80.175980</td>\n",
              "      <td>63.169912</td>\n",
              "      <td>16.705742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>102.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>121.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>801.000000</td>\n",
              "      <td>594.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.330000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>192.375000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>164.900000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>932.000000</td>\n",
              "      <td>730.950000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>23.710000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>272.900000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>185.000000</td>\n",
              "      <td>6.400000</td>\n",
              "      <td>968.000000</td>\n",
              "      <td>779.500000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>34.445000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>350.000000</td>\n",
              "      <td>142.950000</td>\n",
              "      <td>118.300000</td>\n",
              "      <td>192.000000</td>\n",
              "      <td>10.200000</td>\n",
              "      <td>1029.400000</td>\n",
              "      <td>824.000000</td>\n",
              "      <td>56.000000</td>\n",
              "      <td>46.135000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>540.000000</td>\n",
              "      <td>359.400000</td>\n",
              "      <td>200.100000</td>\n",
              "      <td>247.000000</td>\n",
              "      <td>32.200000</td>\n",
              "      <td>1145.000000</td>\n",
              "      <td>992.600000</td>\n",
              "      <td>365.000000</td>\n",
              "      <td>82.600000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Cement  Blast Furnace Slag  ...          Age     Strength\n",
              "count  1030.000000         1030.000000  ...  1030.000000  1030.000000\n",
              "mean    281.167864           73.895825  ...    45.662136    35.817961\n",
              "std     104.506364           86.279342  ...    63.169912    16.705742\n",
              "min     102.000000            0.000000  ...     1.000000     2.330000\n",
              "25%     192.375000            0.000000  ...     7.000000    23.710000\n",
              "50%     272.900000           22.000000  ...    28.000000    34.445000\n",
              "75%     350.000000          142.950000  ...    56.000000    46.135000\n",
              "max     540.000000          359.400000  ...   365.000000    82.600000\n",
              "\n",
              "[8 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCtzAIhqmxVO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28a2ca74-d77b-4882-c49c-72d3b3c85b13"
      },
      "source": [
        "concrete_data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Cement                0\n",
              "Blast Furnace Slag    0\n",
              "Fly Ash               0\n",
              "Water                 0\n",
              "Superplasticizer      0\n",
              "Coarse Aggregate      0\n",
              "Fine Aggregate        0\n",
              "Age                   0\n",
              "Strength              0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buDDmNXrm08N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da3e0009-d67e-44a9-b150-67f8cc66cb09"
      },
      "source": [
        "concrete_data_columns = concrete_data.columns\n",
        "\n",
        "predictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']] # all columns except Strength\n",
        "target = concrete_data['Strength'] # Strength column"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1030,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjpPWAkcnKXd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ec82dc29-8fda-4a54-d270-e4327aa9ea60"
      },
      "source": [
        "predictors_norm = (predictors - predictors.mean()) / predictors.std()\n",
        "predictors_norm.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.476712</td>\n",
              "      <td>-0.856472</td>\n",
              "      <td>-0.846733</td>\n",
              "      <td>-0.916319</td>\n",
              "      <td>-0.620147</td>\n",
              "      <td>0.862735</td>\n",
              "      <td>-1.217079</td>\n",
              "      <td>-0.279597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.476712</td>\n",
              "      <td>-0.856472</td>\n",
              "      <td>-0.846733</td>\n",
              "      <td>-0.916319</td>\n",
              "      <td>-0.620147</td>\n",
              "      <td>1.055651</td>\n",
              "      <td>-1.217079</td>\n",
              "      <td>-0.279597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.491187</td>\n",
              "      <td>0.795140</td>\n",
              "      <td>-0.846733</td>\n",
              "      <td>2.174405</td>\n",
              "      <td>-1.038638</td>\n",
              "      <td>-0.526262</td>\n",
              "      <td>-2.239829</td>\n",
              "      <td>3.551340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.491187</td>\n",
              "      <td>0.795140</td>\n",
              "      <td>-0.846733</td>\n",
              "      <td>2.174405</td>\n",
              "      <td>-1.038638</td>\n",
              "      <td>-0.526262</td>\n",
              "      <td>-2.239829</td>\n",
              "      <td>5.055221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.790075</td>\n",
              "      <td>0.678079</td>\n",
              "      <td>-0.846733</td>\n",
              "      <td>0.488555</td>\n",
              "      <td>-1.038638</td>\n",
              "      <td>0.070492</td>\n",
              "      <td>0.647569</td>\n",
              "      <td>4.976069</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Cement  Blast Furnace Slag  ...  Fine Aggregate       Age\n",
              "0  2.476712           -0.856472  ...       -1.217079 -0.279597\n",
              "1  2.476712           -0.856472  ...       -1.217079 -0.279597\n",
              "2  0.491187            0.795140  ...       -2.239829  3.551340\n",
              "3  0.491187            0.795140  ...       -2.239829  5.055221\n",
              "4 -0.790075            0.678079  ...        0.647569  4.976069\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj9Vm6q3nVR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7196f3d-7393-4ffc-9637-35aa104fffc4"
      },
      "source": [
        "n_cols = predictors_norm.shape[1] # number of predictors\n",
        "n_cols"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF3batksno8c"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDH1UhcHn6Hz"
      },
      "source": [
        "# define regression model\n",
        "def regression_model(num):\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(num, activation='relu', input_shape=(n_cols,)))\n",
        "    model.add(Dense(num, activation='relu'))\n",
        "    model.add(Dense(num, activation='relu'))\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    # compile model\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByB6ecwdoVu-"
      },
      "source": [
        "# build the model\n",
        "model = regression_model(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5Agl5KwoZAW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0debb9d-6254-4023-86e8-530291a7e0df"
      },
      "source": [
        "# fit the model\n",
        "model.fit(predictors_norm, target, validation_split=0.3, epochs=100, verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "23/23 - 0s - loss: 1630.2247 - val_loss: 1110.4183\n",
            "Epoch 2/100\n",
            "23/23 - 0s - loss: 1362.9479 - val_loss: 778.9532\n",
            "Epoch 3/100\n",
            "23/23 - 0s - loss: 729.6199 - val_loss: 245.5069\n",
            "Epoch 4/100\n",
            "23/23 - 0s - loss: 297.2756 - val_loss: 167.8885\n",
            "Epoch 5/100\n",
            "23/23 - 0s - loss: 231.5887 - val_loss: 165.8814\n",
            "Epoch 6/100\n",
            "23/23 - 0s - loss: 213.3221 - val_loss: 158.4589\n",
            "Epoch 7/100\n",
            "23/23 - 0s - loss: 202.1797 - val_loss: 155.4595\n",
            "Epoch 8/100\n",
            "23/23 - 0s - loss: 192.3853 - val_loss: 153.5469\n",
            "Epoch 9/100\n",
            "23/23 - 0s - loss: 184.6979 - val_loss: 152.0002\n",
            "Epoch 10/100\n",
            "23/23 - 0s - loss: 178.4237 - val_loss: 147.3893\n",
            "Epoch 11/100\n",
            "23/23 - 0s - loss: 172.5667 - val_loss: 144.7189\n",
            "Epoch 12/100\n",
            "23/23 - 0s - loss: 167.5667 - val_loss: 143.5790\n",
            "Epoch 13/100\n",
            "23/23 - 0s - loss: 162.3901 - val_loss: 140.1634\n",
            "Epoch 14/100\n",
            "23/23 - 0s - loss: 158.9666 - val_loss: 141.7185\n",
            "Epoch 15/100\n",
            "23/23 - 0s - loss: 154.2629 - val_loss: 138.7423\n",
            "Epoch 16/100\n",
            "23/23 - 0s - loss: 148.4545 - val_loss: 138.7667\n",
            "Epoch 17/100\n",
            "23/23 - 0s - loss: 144.9573 - val_loss: 135.5438\n",
            "Epoch 18/100\n",
            "23/23 - 0s - loss: 142.5873 - val_loss: 136.2772\n",
            "Epoch 19/100\n",
            "23/23 - 0s - loss: 135.9648 - val_loss: 133.2004\n",
            "Epoch 20/100\n",
            "23/23 - 0s - loss: 131.3008 - val_loss: 130.2013\n",
            "Epoch 21/100\n",
            "23/23 - 0s - loss: 126.0380 - val_loss: 131.5437\n",
            "Epoch 22/100\n",
            "23/23 - 0s - loss: 120.4830 - val_loss: 125.7409\n",
            "Epoch 23/100\n",
            "23/23 - 0s - loss: 115.5461 - val_loss: 126.2540\n",
            "Epoch 24/100\n",
            "23/23 - 0s - loss: 110.4865 - val_loss: 125.4309\n",
            "Epoch 25/100\n",
            "23/23 - 0s - loss: 105.7709 - val_loss: 125.9418\n",
            "Epoch 26/100\n",
            "23/23 - 0s - loss: 99.0236 - val_loss: 121.6468\n",
            "Epoch 27/100\n",
            "23/23 - 0s - loss: 95.0335 - val_loss: 120.6564\n",
            "Epoch 28/100\n",
            "23/23 - 0s - loss: 89.8925 - val_loss: 121.2950\n",
            "Epoch 29/100\n",
            "23/23 - 0s - loss: 85.2061 - val_loss: 119.1746\n",
            "Epoch 30/100\n",
            "23/23 - 0s - loss: 81.4766 - val_loss: 120.1679\n",
            "Epoch 31/100\n",
            "23/23 - 0s - loss: 76.6249 - val_loss: 113.6962\n",
            "Epoch 32/100\n",
            "23/23 - 0s - loss: 73.4508 - val_loss: 116.5104\n",
            "Epoch 33/100\n",
            "23/23 - 0s - loss: 70.5280 - val_loss: 114.9515\n",
            "Epoch 34/100\n",
            "23/23 - 0s - loss: 66.7435 - val_loss: 111.8655\n",
            "Epoch 35/100\n",
            "23/23 - 0s - loss: 65.3669 - val_loss: 118.3701\n",
            "Epoch 36/100\n",
            "23/23 - 0s - loss: 62.5067 - val_loss: 110.4226\n",
            "Epoch 37/100\n",
            "23/23 - 0s - loss: 61.0847 - val_loss: 120.7346\n",
            "Epoch 38/100\n",
            "23/23 - 0s - loss: 59.8558 - val_loss: 106.2635\n",
            "Epoch 39/100\n",
            "23/23 - 0s - loss: 56.9063 - val_loss: 106.2182\n",
            "Epoch 40/100\n",
            "23/23 - 0s - loss: 54.1810 - val_loss: 114.3907\n",
            "Epoch 41/100\n",
            "23/23 - 0s - loss: 52.2630 - val_loss: 119.0591\n",
            "Epoch 42/100\n",
            "23/23 - 0s - loss: 51.0784 - val_loss: 120.5402\n",
            "Epoch 43/100\n",
            "23/23 - 0s - loss: 51.7010 - val_loss: 118.4968\n",
            "Epoch 44/100\n",
            "23/23 - 0s - loss: 48.2749 - val_loss: 128.0191\n",
            "Epoch 45/100\n",
            "23/23 - 0s - loss: 47.6689 - val_loss: 112.1924\n",
            "Epoch 46/100\n",
            "23/23 - 0s - loss: 45.0223 - val_loss: 118.7747\n",
            "Epoch 47/100\n",
            "23/23 - 0s - loss: 44.6132 - val_loss: 117.1359\n",
            "Epoch 48/100\n",
            "23/23 - 0s - loss: 42.3670 - val_loss: 118.4398\n",
            "Epoch 49/100\n",
            "23/23 - 0s - loss: 41.7909 - val_loss: 115.6773\n",
            "Epoch 50/100\n",
            "23/23 - 0s - loss: 40.7263 - val_loss: 114.9973\n",
            "Epoch 51/100\n",
            "23/23 - 0s - loss: 40.5855 - val_loss: 123.2812\n",
            "Epoch 52/100\n",
            "23/23 - 0s - loss: 38.4599 - val_loss: 124.0529\n",
            "Epoch 53/100\n",
            "23/23 - 0s - loss: 37.9520 - val_loss: 123.3829\n",
            "Epoch 54/100\n",
            "23/23 - 0s - loss: 36.8903 - val_loss: 114.3484\n",
            "Epoch 55/100\n",
            "23/23 - 0s - loss: 36.0219 - val_loss: 114.0460\n",
            "Epoch 56/100\n",
            "23/23 - 0s - loss: 35.7502 - val_loss: 108.4599\n",
            "Epoch 57/100\n",
            "23/23 - 0s - loss: 34.9816 - val_loss: 116.9636\n",
            "Epoch 58/100\n",
            "23/23 - 0s - loss: 34.0541 - val_loss: 130.0092\n",
            "Epoch 59/100\n",
            "23/23 - 0s - loss: 33.4376 - val_loss: 119.6924\n",
            "Epoch 60/100\n",
            "23/23 - 0s - loss: 34.3151 - val_loss: 136.6776\n",
            "Epoch 61/100\n",
            "23/23 - 0s - loss: 33.0198 - val_loss: 143.1910\n",
            "Epoch 62/100\n",
            "23/23 - 0s - loss: 31.9381 - val_loss: 128.7373\n",
            "Epoch 63/100\n",
            "23/23 - 0s - loss: 31.6520 - val_loss: 129.4915\n",
            "Epoch 64/100\n",
            "23/23 - 0s - loss: 32.1616 - val_loss: 128.4463\n",
            "Epoch 65/100\n",
            "23/23 - 0s - loss: 30.7155 - val_loss: 141.7812\n",
            "Epoch 66/100\n",
            "23/23 - 0s - loss: 31.6573 - val_loss: 136.2798\n",
            "Epoch 67/100\n",
            "23/23 - 0s - loss: 30.3693 - val_loss: 141.2126\n",
            "Epoch 68/100\n",
            "23/23 - 0s - loss: 28.5994 - val_loss: 135.3325\n",
            "Epoch 69/100\n",
            "23/23 - 0s - loss: 29.6346 - val_loss: 131.6657\n",
            "Epoch 70/100\n",
            "23/23 - 0s - loss: 28.6603 - val_loss: 140.9430\n",
            "Epoch 71/100\n",
            "23/23 - 0s - loss: 27.9924 - val_loss: 129.6068\n",
            "Epoch 72/100\n",
            "23/23 - 0s - loss: 28.5827 - val_loss: 118.2232\n",
            "Epoch 73/100\n",
            "23/23 - 0s - loss: 29.1089 - val_loss: 151.3249\n",
            "Epoch 74/100\n",
            "23/23 - 0s - loss: 28.2394 - val_loss: 148.9112\n",
            "Epoch 75/100\n",
            "23/23 - 0s - loss: 26.6573 - val_loss: 142.0953\n",
            "Epoch 76/100\n",
            "23/23 - 0s - loss: 26.4735 - val_loss: 149.6692\n",
            "Epoch 77/100\n",
            "23/23 - 0s - loss: 25.8957 - val_loss: 151.2418\n",
            "Epoch 78/100\n",
            "23/23 - 0s - loss: 25.7139 - val_loss: 136.1905\n",
            "Epoch 79/100\n",
            "23/23 - 0s - loss: 25.3060 - val_loss: 148.0152\n",
            "Epoch 80/100\n",
            "23/23 - 0s - loss: 24.7902 - val_loss: 151.4830\n",
            "Epoch 81/100\n",
            "23/23 - 0s - loss: 24.7467 - val_loss: 154.5177\n",
            "Epoch 82/100\n",
            "23/23 - 0s - loss: 24.5785 - val_loss: 149.8667\n",
            "Epoch 83/100\n",
            "23/23 - 0s - loss: 24.1041 - val_loss: 167.6814\n",
            "Epoch 84/100\n",
            "23/23 - 0s - loss: 24.1250 - val_loss: 161.9162\n",
            "Epoch 85/100\n",
            "23/23 - 0s - loss: 23.8272 - val_loss: 160.3119\n",
            "Epoch 86/100\n",
            "23/23 - 0s - loss: 24.0461 - val_loss: 155.8314\n",
            "Epoch 87/100\n",
            "23/23 - 0s - loss: 23.4180 - val_loss: 173.7951\n",
            "Epoch 88/100\n",
            "23/23 - 0s - loss: 22.6303 - val_loss: 160.2571\n",
            "Epoch 89/100\n",
            "23/23 - 0s - loss: 23.2927 - val_loss: 151.0213\n",
            "Epoch 90/100\n",
            "23/23 - 0s - loss: 22.8509 - val_loss: 163.9136\n",
            "Epoch 91/100\n",
            "23/23 - 0s - loss: 22.1286 - val_loss: 164.6846\n",
            "Epoch 92/100\n",
            "23/23 - 0s - loss: 21.6805 - val_loss: 182.3248\n",
            "Epoch 93/100\n",
            "23/23 - 0s - loss: 21.6588 - val_loss: 167.2662\n",
            "Epoch 94/100\n",
            "23/23 - 0s - loss: 21.5072 - val_loss: 178.3926\n",
            "Epoch 95/100\n",
            "23/23 - 0s - loss: 20.8023 - val_loss: 187.2692\n",
            "Epoch 96/100\n",
            "23/23 - 0s - loss: 21.4804 - val_loss: 171.5616\n",
            "Epoch 97/100\n",
            "23/23 - 0s - loss: 20.9218 - val_loss: 173.7645\n",
            "Epoch 98/100\n",
            "23/23 - 0s - loss: 20.3950 - val_loss: 178.5720\n",
            "Epoch 99/100\n",
            "23/23 - 0s - loss: 20.2767 - val_loss: 192.2558\n",
            "Epoch 100/100\n",
            "23/23 - 0s - loss: 20.0007 - val_loss: 193.9709\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f50eb39c278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC0YkMZwon6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcb38ad1-8901-4146-d173-a5aad12868d5"
      },
      "source": [
        "# build the model\n",
        "model2 = regression_model(100)\n",
        "\n",
        "# fit the model\n",
        "model2.fit(predictors_norm, target, validation_split=0.3, epochs=200, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1538.3717 - val_loss: 872.6944\n",
            "Epoch 2/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 726.2414 - val_loss: 223.5866\n",
            "Epoch 3/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 281.2858 - val_loss: 182.5638\n",
            "Epoch 4/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 219.7775 - val_loss: 181.1687\n",
            "Epoch 5/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 202.1087 - val_loss: 179.9018\n",
            "Epoch 6/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 186.2440 - val_loss: 174.4267\n",
            "Epoch 7/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 174.0393 - val_loss: 178.4235\n",
            "Epoch 8/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 164.9341 - val_loss: 169.7402\n",
            "Epoch 9/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 154.3817 - val_loss: 174.7151\n",
            "Epoch 10/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 147.6935 - val_loss: 176.1920\n",
            "Epoch 11/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 136.8680 - val_loss: 176.8085\n",
            "Epoch 12/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 127.3919 - val_loss: 170.5273\n",
            "Epoch 13/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 114.7946 - val_loss: 188.9179\n",
            "Epoch 14/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 105.3219 - val_loss: 210.6185\n",
            "Epoch 15/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 98.7523 - val_loss: 180.9690\n",
            "Epoch 16/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 85.0250 - val_loss: 173.9204\n",
            "Epoch 17/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 77.1552 - val_loss: 197.7860\n",
            "Epoch 18/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 68.6867 - val_loss: 175.8715\n",
            "Epoch 19/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 61.8528 - val_loss: 209.7075\n",
            "Epoch 20/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 57.6011 - val_loss: 204.3943\n",
            "Epoch 21/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 51.9893 - val_loss: 200.1051\n",
            "Epoch 22/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 49.3354 - val_loss: 206.4649\n",
            "Epoch 23/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 45.5556 - val_loss: 206.0765\n",
            "Epoch 24/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 43.2647 - val_loss: 196.2416\n",
            "Epoch 25/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 41.0116 - val_loss: 176.7790\n",
            "Epoch 26/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 39.9485 - val_loss: 183.7539\n",
            "Epoch 27/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 38.4350 - val_loss: 193.9048\n",
            "Epoch 28/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 38.2491 - val_loss: 207.3503\n",
            "Epoch 29/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 37.8742 - val_loss: 163.6310\n",
            "Epoch 30/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 36.8007 - val_loss: 212.6169\n",
            "Epoch 31/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 34.1114 - val_loss: 205.8576\n",
            "Epoch 32/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 32.7969 - val_loss: 199.0746\n",
            "Epoch 33/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 31.6437 - val_loss: 195.0157\n",
            "Epoch 34/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 30.0463 - val_loss: 210.6042\n",
            "Epoch 35/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 30.0478 - val_loss: 207.4054\n",
            "Epoch 36/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 29.5880 - val_loss: 193.9134\n",
            "Epoch 37/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 27.7343 - val_loss: 214.5905\n",
            "Epoch 38/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 28.2952 - val_loss: 181.0664\n",
            "Epoch 39/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 27.6175 - val_loss: 211.3896\n",
            "Epoch 40/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 26.3036 - val_loss: 202.8976\n",
            "Epoch 41/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 26.3209 - val_loss: 178.2342\n",
            "Epoch 42/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 26.5573 - val_loss: 195.0672\n",
            "Epoch 43/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 23.5238 - val_loss: 199.8776\n",
            "Epoch 44/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 23.7517 - val_loss: 188.0171\n",
            "Epoch 45/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 22.7729 - val_loss: 184.2797\n",
            "Epoch 46/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 23.8253 - val_loss: 179.7735\n",
            "Epoch 47/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 22.4691 - val_loss: 181.2154\n",
            "Epoch 48/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 23.4553 - val_loss: 183.6940\n",
            "Epoch 49/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 21.6121 - val_loss: 184.5477\n",
            "Epoch 50/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 20.8414 - val_loss: 168.4072\n",
            "Epoch 51/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 22.4068 - val_loss: 193.6658\n",
            "Epoch 52/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 19.4474 - val_loss: 175.4356\n",
            "Epoch 53/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 19.5133 - val_loss: 180.1051\n",
            "Epoch 54/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 19.3589 - val_loss: 156.8135\n",
            "Epoch 55/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 19.3335 - val_loss: 171.1531\n",
            "Epoch 56/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 18.9872 - val_loss: 157.4281\n",
            "Epoch 57/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 19.3623 - val_loss: 182.0163\n",
            "Epoch 58/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 17.6873 - val_loss: 183.7097\n",
            "Epoch 59/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 18.4423 - val_loss: 165.6507\n",
            "Epoch 60/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 17.2968 - val_loss: 173.9592\n",
            "Epoch 61/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 17.3122 - val_loss: 165.2771\n",
            "Epoch 62/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 17.3891 - val_loss: 178.3985\n",
            "Epoch 63/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 16.9038 - val_loss: 173.7379\n",
            "Epoch 64/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 15.7746 - val_loss: 160.0473\n",
            "Epoch 65/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 15.7508 - val_loss: 179.4159\n",
            "Epoch 66/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 16.2310 - val_loss: 172.7073\n",
            "Epoch 67/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 16.0345 - val_loss: 161.9223\n",
            "Epoch 68/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 15.4552 - val_loss: 180.1989\n",
            "Epoch 69/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 15.1478 - val_loss: 182.3464\n",
            "Epoch 70/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 15.0673 - val_loss: 163.7329\n",
            "Epoch 71/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 14.5596 - val_loss: 193.9346\n",
            "Epoch 72/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 15.3848 - val_loss: 154.7466\n",
            "Epoch 73/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 15.1627 - val_loss: 170.2298\n",
            "Epoch 74/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 15.2571 - val_loss: 178.7810\n",
            "Epoch 75/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 14.6294 - val_loss: 166.3004\n",
            "Epoch 76/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 16.1044 - val_loss: 198.8622\n",
            "Epoch 77/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 16.1032 - val_loss: 185.0337\n",
            "Epoch 78/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 14.4766 - val_loss: 184.7685\n",
            "Epoch 79/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 13.4490 - val_loss: 190.3097\n",
            "Epoch 80/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 14.3129 - val_loss: 182.3950\n",
            "Epoch 81/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 15.1025 - val_loss: 166.4487\n",
            "Epoch 82/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 13.6084 - val_loss: 184.8237\n",
            "Epoch 83/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 15.0039 - val_loss: 156.9221\n",
            "Epoch 84/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 12.8479 - val_loss: 189.5915\n",
            "Epoch 85/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 13.1416 - val_loss: 200.7495\n",
            "Epoch 86/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 12.7696 - val_loss: 152.9586\n",
            "Epoch 87/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 13.5240 - val_loss: 186.5840\n",
            "Epoch 88/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 14.4171 - val_loss: 180.5575\n",
            "Epoch 89/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 12.7565 - val_loss: 198.0379\n",
            "Epoch 90/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 13.1590 - val_loss: 162.5550\n",
            "Epoch 91/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 13.3996 - val_loss: 187.1563\n",
            "Epoch 92/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 12.5826 - val_loss: 181.6192\n",
            "Epoch 93/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 12.5902 - val_loss: 181.9356\n",
            "Epoch 94/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 13.2887 - val_loss: 161.9619\n",
            "Epoch 95/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 12.1067 - val_loss: 205.9168\n",
            "Epoch 96/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 12.2828 - val_loss: 195.0975\n",
            "Epoch 97/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 12.8973 - val_loss: 196.8653\n",
            "Epoch 98/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 12.1706 - val_loss: 182.2436\n",
            "Epoch 99/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 11.5390 - val_loss: 190.9244\n",
            "Epoch 100/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 11.6023 - val_loss: 199.5762\n",
            "Epoch 101/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 12.0175 - val_loss: 162.2532\n",
            "Epoch 102/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 11.9651 - val_loss: 163.7774\n",
            "Epoch 103/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 12.3396 - val_loss: 188.8478\n",
            "Epoch 104/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 11.7840 - val_loss: 181.3717\n",
            "Epoch 105/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 12.5678 - val_loss: 162.2720\n",
            "Epoch 106/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 12.6567 - val_loss: 196.7029\n",
            "Epoch 107/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 14.0906 - val_loss: 174.5101\n",
            "Epoch 108/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 13.5510 - val_loss: 188.4846\n",
            "Epoch 109/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 12.6032 - val_loss: 196.8755\n",
            "Epoch 110/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 11.9992 - val_loss: 189.2506\n",
            "Epoch 111/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 11.9301 - val_loss: 169.0167\n",
            "Epoch 112/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 11.2691 - val_loss: 181.5966\n",
            "Epoch 113/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 11.4681 - val_loss: 189.4798\n",
            "Epoch 114/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 11.6680 - val_loss: 156.2587\n",
            "Epoch 115/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 11.2373 - val_loss: 186.0739\n",
            "Epoch 116/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 11.8432 - val_loss: 194.7305\n",
            "Epoch 117/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 10.7802 - val_loss: 192.8246\n",
            "Epoch 118/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 11.1757 - val_loss: 192.4616\n",
            "Epoch 119/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 10.7071 - val_loss: 193.2060\n",
            "Epoch 120/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 10.2639 - val_loss: 196.3347\n",
            "Epoch 121/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 10.8070 - val_loss: 188.9442\n",
            "Epoch 122/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 11.4946 - val_loss: 182.9543\n",
            "Epoch 123/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 10.9614 - val_loss: 179.8349\n",
            "Epoch 124/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 10.5554 - val_loss: 206.0185\n",
            "Epoch 125/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 10.4650 - val_loss: 214.3088\n",
            "Epoch 126/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 11.4838 - val_loss: 194.6283\n",
            "Epoch 127/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 10.2892 - val_loss: 194.6189\n",
            "Epoch 128/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 9.8295 - val_loss: 197.2772\n",
            "Epoch 129/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 11.2738 - val_loss: 209.1251\n",
            "Epoch 130/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 10.7387 - val_loss: 230.5788\n",
            "Epoch 131/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 10.4509 - val_loss: 205.0856\n",
            "Epoch 132/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 10.4219 - val_loss: 173.0211\n",
            "Epoch 133/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 11.3108 - val_loss: 193.5896\n",
            "Epoch 134/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 10.5591 - val_loss: 204.7643\n",
            "Epoch 135/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 10.4361 - val_loss: 214.4044\n",
            "Epoch 136/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 9.7821 - val_loss: 204.1954\n",
            "Epoch 137/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 10.2709 - val_loss: 167.8440\n",
            "Epoch 138/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 9.8186 - val_loss: 208.1129\n",
            "Epoch 139/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 10.0632 - val_loss: 191.7786\n",
            "Epoch 140/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 10.7184 - val_loss: 202.6728\n",
            "Epoch 141/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 10.2613 - val_loss: 201.9032\n",
            "Epoch 142/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 10.1239 - val_loss: 225.3324\n",
            "Epoch 143/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 10.3266 - val_loss: 211.1393\n",
            "Epoch 144/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 9.8301 - val_loss: 199.0417\n",
            "Epoch 145/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 11.2561 - val_loss: 172.8181\n",
            "Epoch 146/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 12.0228 - val_loss: 197.8372\n",
            "Epoch 147/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 9.7156 - val_loss: 172.8842\n",
            "Epoch 148/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 9.2149 - val_loss: 193.6375\n",
            "Epoch 149/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 9.2371 - val_loss: 188.0501\n",
            "Epoch 150/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 9.6757 - val_loss: 181.4686\n",
            "Epoch 151/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 9.6746 - val_loss: 218.5477\n",
            "Epoch 152/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 10.2829 - val_loss: 180.1824\n",
            "Epoch 153/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 10.0949 - val_loss: 173.1009\n",
            "Epoch 154/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 10.0332 - val_loss: 189.2764\n",
            "Epoch 155/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 8.9177 - val_loss: 197.9686\n",
            "Epoch 156/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 9.3246 - val_loss: 203.1151\n",
            "Epoch 157/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 9.5847 - val_loss: 209.6118\n",
            "Epoch 158/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 9.7613 - val_loss: 227.5699\n",
            "Epoch 159/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 10.5553 - val_loss: 205.4027\n",
            "Epoch 160/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 9.4586 - val_loss: 179.4338\n",
            "Epoch 161/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 10.6906 - val_loss: 177.4772\n",
            "Epoch 162/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 10.4730 - val_loss: 189.4177\n",
            "Epoch 163/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 9.7465 - val_loss: 206.4785\n",
            "Epoch 164/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 9.5060 - val_loss: 190.0800\n",
            "Epoch 165/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 9.0246 - val_loss: 205.4894\n",
            "Epoch 166/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 9.2991 - val_loss: 176.9247\n",
            "Epoch 167/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 9.3825 - val_loss: 220.2796\n",
            "Epoch 168/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 10.0931 - val_loss: 219.3200\n",
            "Epoch 169/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 9.0132 - val_loss: 204.0350\n",
            "Epoch 170/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 9.1618 - val_loss: 185.0684\n",
            "Epoch 171/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 9.8465 - val_loss: 177.9521\n",
            "Epoch 172/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 8.8740 - val_loss: 227.3969\n",
            "Epoch 173/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 9.2765 - val_loss: 207.7937\n",
            "Epoch 174/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 8.6073 - val_loss: 198.6073\n",
            "Epoch 175/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 9.2630 - val_loss: 209.8927\n",
            "Epoch 176/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 9.2477 - val_loss: 212.4398\n",
            "Epoch 177/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 8.7635 - val_loss: 187.1028\n",
            "Epoch 178/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 9.1279 - val_loss: 198.5551\n",
            "Epoch 179/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 9.5955 - val_loss: 184.5301\n",
            "Epoch 180/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 9.0541 - val_loss: 176.7552\n",
            "Epoch 181/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 8.6201 - val_loss: 196.6723\n",
            "Epoch 182/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 8.6838 - val_loss: 205.8767\n",
            "Epoch 183/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 9.7001 - val_loss: 172.0897\n",
            "Epoch 184/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 8.6071 - val_loss: 188.5233\n",
            "Epoch 185/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 8.9202 - val_loss: 207.0250\n",
            "Epoch 186/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 8.5203 - val_loss: 203.4830\n",
            "Epoch 187/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 8.8468 - val_loss: 228.8999\n",
            "Epoch 188/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 8.9696 - val_loss: 203.6767\n",
            "Epoch 189/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 9.1192 - val_loss: 229.2372\n",
            "Epoch 190/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 9.1965 - val_loss: 230.3273\n",
            "Epoch 191/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 8.5082 - val_loss: 188.3284\n",
            "Epoch 192/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 9.8512 - val_loss: 205.4066\n",
            "Epoch 193/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 8.3152 - val_loss: 182.9704\n",
            "Epoch 194/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 8.8837 - val_loss: 199.2475\n",
            "Epoch 195/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 8.5221 - val_loss: 195.6873\n",
            "Epoch 196/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 8.4986 - val_loss: 195.3088\n",
            "Epoch 197/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 8.4337 - val_loss: 218.2764\n",
            "Epoch 198/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 8.1507 - val_loss: 193.6308\n",
            "Epoch 199/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 8.5969 - val_loss: 217.8368\n",
            "Epoch 200/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 8.1441 - val_loss: 222.5492\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f50ea250f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}